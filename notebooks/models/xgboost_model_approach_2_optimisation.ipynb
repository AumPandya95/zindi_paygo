{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "mod_path = os.path.join(Path.cwd().parent.parent)\n",
    "if mod_path not in sys.path:\n",
    "    sys.path.append(mod_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from typing import List, Tuple\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "from src.data import *\n",
    "from src.features.utils import *\n",
    "from src.model.tree_based import ModelXgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with shape 28007, 33 [transaction related features]\n",
    "train = pd.read_csv('../../data/processed/train.csv')\n",
    "test_set = pd.read_csv('../../data/processed/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.drop(columns = 'Unnamed: 0', inplace=True)\n",
    "test_set.drop(columns = 'Unnamed: 0', inplace=True)\n",
    "# test_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28007, 35)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.162994965544328\n"
     ]
    }
   ],
   "source": [
    "# Region has certain NaN values which might cause issues while encoding\n",
    "# As total NaNs constitute ~5% of the data (1446) we remove it as of now\n",
    "print(train['Region'].isna().sum() / train.shape[0] * 100)\n",
    "train.dropna(subset=['Region'], how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slide_variable_window(\n",
    "    predictor_array: pd.DataFrame,\n",
    "    var_to_add: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    predictor_array.drop(columns=['b5'], inplace=True)  # We drop the first payment\n",
    "    predictor_array.rename(columns={'b1': 'b2', 'b2':'b3', 'b3': 'b4', 'b4': 'b5'}, inplace=True)\n",
    "    predictor_array.insert(loc=0, column='b1', value=var_to_add.values)  # And add the new variable (mn)\n",
    "    \n",
    "    return predictor_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_payment_history_df = train[[\"ID\", \"SplitPaymentsHistory\"]]\n",
    "# id_arr = train[[\"ID\"]]\n",
    "\n",
    "target = train[['m1', 'm2', 'm3', 'm4', 'm5', 'm6']]\n",
    "train_arr = train.drop(columns=['m1', 'm2', 'm3', 'm4', 'm5', 'm6', \n",
    "                                'SplitPaymentsHistory',\n",
    "                                'ExpectedTermDate', \n",
    "                                'FirstPaymentDate',\n",
    "                                'LastPaymentDate'])\n",
    "test_set.drop(columns=['SplitPaymentsHistory',\n",
    "                       'ExpectedTermDate', \n",
    "                       'FirstPaymentDate',\n",
    "                       'LastPaymentDate'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_data_with_sliding_approach(data_without_target: pd.DataFrame,\n",
    "                                      target_data: pd.DataFrame):\n",
    "    frame = pd.DataFrame(None)\n",
    "    new_df = deepcopy(data_without_target)\n",
    "    target_df = pd.DataFrame(None)\n",
    "    target_features = target_data.columns.tolist()\n",
    "    for itr, col in enumerate(target_features):\n",
    "        if itr == 0:\n",
    "            target_df = pd.concat([target_df, target_data[[col]]])\n",
    "            frame = pd.concat([frame, data_without_target])\n",
    "        else:\n",
    "            filter_df = new_df[['b1', 'b2', 'b3', 'b4', 'b5']]  # Intermediate df\n",
    "            new_df.drop(columns=['b1', 'b2', 'b3', 'b4', 'b5'], inplace=True)\n",
    "            concatinating_df = slide_variable_window(predictor_array=filter_df, \n",
    "                                                     var_to_add=target_data[[target_features[itr-1]]])\n",
    "            new_df = pd.concat([new_df, concatinating_df], axis=1)  # We add the newly created columns\n",
    "            target_df = pd.concat([target_df, target_data[[col]]])\n",
    "            frame = pd.concat([frame, new_df])\n",
    "#             print(new_df.shape)\n",
    "\n",
    "    target_df = pd.DataFrame(target_df.sum(axis=1).astype(int), columns=['target'])\n",
    "#     print(frame.shape)  # Should be 6 * original data's no. of rows\n",
    "    \n",
    "    frame.reset_index(drop=True, inplace=True)\n",
    "    target_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return frame, target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 10\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_arr, target, test_size=0.45, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model train on initial hp :: Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_drop(full_array, data_type, tr_encoder=None):\n",
    "    # print(tr_encoder)\n",
    "    categorical_array = full_array[full_array.select_dtypes(exclude=['number']).columns]\n",
    "    numerical_array = full_array.drop(columns=full_array.select_dtypes(exclude=['number']).columns)\n",
    "    # print(categorical_array.columns)\n",
    "    encoded_array, encoder = one_hot_encoding(\n",
    "        categorical_frame=categorical_array, \n",
    "        type_of_data=data_type,\n",
    "        fitted_encoder=tr_encoder,\n",
    "        conv=True,\n",
    "        drop='first',\n",
    "        handle_unknown=\"error\"\n",
    "    )\n",
    "    # print(encoded_array.columns)\n",
    "    final_array = pd.concat([numerical_array.reset_index(drop=True), encoded_array.reset_index(drop=True)], axis=1)\n",
    "    final_array.index = numerical_array.index\n",
    "    # print(final_array.shape)\n",
    "    \n",
    "    return numerical_array, encoded_array, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_top_features(\n",
    "    features,\n",
    "    feature_scores,\n",
    "    cut_off_score=0.8\n",
    ") -> Tuple[List[List], List[List]]:\n",
    "    frame = pd.DataFrame([feature_scores], columns=features, index=['gain']).T\n",
    "    frame.sort_values(by=['gain'], ascending=False, inplace=True)\n",
    "    frame['cum_gain'] = frame['gain'].cumsum()\n",
    "    \n",
    "    feature_list = list(frame[frame['cum_gain'] <= cut_off_score].index)\n",
    "    feature_scores_list = frame[frame['cum_gain'] <= cut_off_score]['gain'].tolist()\n",
    "    \n",
    "    return feature_list, feature_scores_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_important_feature_scores(\n",
    "    feature_corpus: List[List],\n",
    "    score_corpus: List[List]\n",
    ") -> pd.DataFrame:\n",
    "    imp_column = dict()\n",
    "    imp_col = dict()\n",
    "    for feature_list, score_list in zip(feature_corpus, score_corpus):\n",
    "        for feature_names, scores in zip(feature_list, score_list):\n",
    "            if feature_names not in imp_column.keys():\n",
    "                imp_column[feature_names] = 1\n",
    "#                 imp_col[feature_names] = [scores]\n",
    "            else:\n",
    "                imp_column[feature_names] += 1\n",
    "#                 imp_col[feature_names].append(scores)\n",
    "    \n",
    "    important_features_df = pd.DataFrame(imp_column, index=['frequency']).T\n",
    "#     important_scores = pd.DataFrame(imp_col)\n",
    "    important_features_df['appearance_ratio'] = important_features_df['frequency'] / 100\n",
    "    important_features_df.sort_values(by=['frequency'], ascending=False, inplace=True)\n",
    "#     important_features_df['cum_score'] = important_features_df['score'].cumsum()\n",
    "    \n",
    "    return important_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Preparing the TRAIN data for approach two and fitting the model\n",
    "train_data, target_frame = create_data_with_sliding_approach(data_without_target=X_train, \n",
    "                                                             target_data=y_train)\n",
    "id_array = train_data[[\"ID\"]]\n",
    "train_data.drop(columns=[\"ID\"], inplace=True)\n",
    "og_frame, encoded_train, encoder_model = encode_and_drop(train_data, \"train\", None)\n",
    "\n",
    "# encoded_train = pd.concat([encoded_train, train_data[['b1', 'b2', 'b3', 'b4', 'b5']]], axis=1)\n",
    "print(encoded_train.shape)\n",
    "\n",
    "np.random.seed(0)\n",
    "r = np.random.randint(1, 100, 100)\n",
    "f_i_list = []\n",
    "f_i_features = []\n",
    "for i, _seed in enumerate(r):\n",
    "    sample= encoded_train.sample(n=20000, replace=False, random_state=_seed)\n",
    "    target_f = target_frame[target_frame.index.isin(sample.index)]\n",
    "    model_two_obj = ModelXgBoost(train_array=sample, \n",
    "                                 train_target=target_f)\n",
    "    model_two_obj.train_model()  # Default h.params (Checkout the code)\n",
    "    model_two = model_two_obj.trained_model\n",
    "    f_i = model_two.feature_importances_\n",
    "    f_i_cols = sample.columns\n",
    "    variables, variable_scores = get_top_features(features=f_i_cols, feature_scores=f_i, cut_off_score=0.8)\n",
    "    f_i_list.append(variable_scores)\n",
    "    f_i_features.append(variables)\n",
    "\n",
    "important_features_df = get_important_feature_scores(feature_corpus=f_i_features, score_corpus=f_i_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "important_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# important_features_df.to_csv('../../submissions/important_features_total_gain_with_80_cutoff.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.figure(figsize=(15,10))\n",
    "pyplot.bar(important_features_df.index, important_features_df['frequency'])\n",
    "# xgb.plot_importance(model_two)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature_selection = list(important_features_df[important_features_df['cum_score'] <= 0.8].index)\n",
    "feature_selection = [\n",
    "    'b1',\n",
    "    'Business',\n",
    "    'Labourer',\n",
    "    'TotalContractValue',\n",
    "    'Western',\n",
    "    'Mount Kenya Region',\n",
    "    'Female',\n",
    "    'Coast Region',\n",
    "    'Nyanza',\n",
    "    'Farmer',\n",
    "    'Other',\n",
    "    'South Rift',\n",
    "    'stddev_amt_paid',\n",
    "    'Teacher',\n",
    "    'Nairobi Region',\n",
    "    'DaysOnDeposit',\n",
    "    'min_amt_paid',\n",
    "    'Term',\n",
    "    'b4',\n",
    "    'mean_amt_paid',\n",
    "    'b2',\n",
    "    'Age',\n",
    "    'amount_paid',\n",
    "    'nb_payments',\n",
    "    'median_amt_paid',\n",
    "    'max_amt_paid',\n",
    "    'Deposit',\n",
    "    'b3',\n",
    "    'percent_amt_paid',\n",
    "    'b5'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rate\": [0.05, 0.1, 0.15, 0.2, 0.25, 0.3],\n",
    "    \"max_depth\": [3, 4, 5, 6, 8, 10, 12, 13],\n",
    "    \"min_child_weight\": [1, 3, 5, 7],\n",
    "    \"gamma\": [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    \"colsample_bytree\": [0.3, 0.4, 0.5, 0.7]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aumaron/.local/share/virtualenvs/zindi_payg-FXkRANRI/lib/python3.9/site-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "/home/aumaron/.local/share/virtualenvs/zindi_payg-FXkRANRI/lib/python3.9/site-packages/pandas/core/frame.py:4441: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    }
   ],
   "source": [
    "# Preparing the TRAIN data for approach two and fitting the model\n",
    "train_data, target_frame = create_data_with_sliding_approach(data_without_target=X_train, \n",
    "                                                             target_data=y_train)\n",
    "id_array = train_data[[\"ID\"]]\n",
    "train_data.drop(columns=[\"ID\"], inplace=True)\n",
    "og_frame, encoded_train, encoder_model = encode_and_drop(train_data, \"train\", None)\n",
    "# encoded_train = encoded_train[feature_selection]\n",
    "encoded_train = pd.concat([encoded_train, train_data[['b1', 'b2', 'b3', 'b4', 'b5']]], axis=1)\n",
    "# print(encoded_train.shape)\n",
    "\n",
    "model_two_obj = ModelXgBoost(train_array=encoded_train,\n",
    "                             train_target=target_frame)\n",
    "model_two_obj.train_model()  # Default h.params (Checkout the code)\n",
    "model_two = model_two_obj.trained_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction using Model :: Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aumaron/.local/share/virtualenvs/zindi_payg-FXkRANRI/lib/python3.9/site-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "test_id_array = X_test[[\"ID\"]]\n",
    "X_test.drop(columns=[\"ID\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aumaron/.local/share/virtualenvs/zindi_payg-FXkRANRI/lib/python3.9/site-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "/home/aumaron/.local/share/virtualenvs/zindi_payg-FXkRANRI/lib/python3.9/site-packages/pandas/core/frame.py:4441: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'m1_pred': array([1312.5391 ,  724.73895,   75.12879, ..., 1530.3356 , 1458.8456 ,\n",
       "        1198.9733 ], dtype=float32),\n",
       " 'm2_pred': array([1294.2546  ,  679.0865  ,  111.778114, ..., 1468.2092  ,\n",
       "        1381.8628  , 1022.49854 ], dtype=float32),\n",
       " 'm3_pred': array([1196.0316 ,  687.7618 ,  105.51476, ..., 1437.452  , 1333.7384 ,\n",
       "         899.16205], dtype=float32),\n",
       " 'm4_pred': array([1116.6182 ,  661.09955,  130.85216, ..., 1399.297  , 1332.8679 ,\n",
       "         714.56696], dtype=float32),\n",
       " 'm5_pred': array([1033.0432,  626.6238,  145.245 , ..., 1327.018 , 1229.2456,\n",
       "         668.316 ], dtype=float32),\n",
       " 'm6_pred': array([1018.6896 ,  625.56354,  191.08568, ..., 1263.2454 , 1115.807  ,\n",
       "         659.57086], dtype=float32)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding and re-attaching using train encoding model\n",
    "og_frame_test, encoded_test, encoder_model = encode_and_drop(X_test, \"test\", encoder_model)\n",
    "encoded_test = pd.concat([encoded_test, X_test[['b1', 'b2', 'b3', 'b4', 'b5']].reset_index(drop=True)], axis=1)\n",
    "\n",
    "predict_dict = dict()\n",
    "# encoded_test = encoded_test[feature_selection]\n",
    "for col_no, predict_col in enumerate(y_test.columns):\n",
    "    predict_dict[f\"m{col_no+1}_pred\"] = model_two.predict(encoded_test)\n",
    "    int_df = encoded_test[['b1', 'b2', 'b3', 'b4', 'b5']]\n",
    "    encoded_test.drop(columns=['b1', 'b2', 'b3', 'b4', 'b5'], inplace=True)\n",
    "    concatinating_df = slide_variable_window(predictor_array=int_df, \n",
    "                                             var_to_add=pd.DataFrame(predict_dict[f\"m{col_no+1}_pred\"]))\n",
    "    encoded_test = pd.concat([encoded_test, concatinating_df], axis=1)  # We add the newly created columns\n",
    "    \n",
    "predict_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-307.70688\n",
      "-115.81817\n",
      "-2.2030687\n",
      "-238.54391\n",
      "-505.05783\n",
      "-338.21112\n",
      "-76.991646\n",
      "-33.419613\n",
      "-124.288864\n",
      "-86.89496\n",
      "-186.09164\n",
      "-446.4476\n",
      "-271.30228\n",
      "-178.38742\n",
      "-38.82201\n",
      "-282.02374\n",
      "-125.88486\n",
      "-85.18992\n",
      "-132.84639\n",
      "-13.080331\n",
      "-505.47446\n",
      "-229.82645\n",
      "-141.85376\n",
      "-2309.8667\n",
      "-89.62878\n",
      "-482.87445\n",
      "-2683.338\n",
      "-70.400826\n",
      "-695.3642\n",
      "-264.89954\n",
      "-9.703663\n",
      "-115.495186\n",
      "-73.09885\n",
      "-564.45026\n",
      "-14.2245865\n",
      "-7.605969\n",
      "-32.56536\n",
      "-172.33122\n",
      "-119.249825\n",
      "-609.5148\n",
      "-403.70853\n",
      "-435.63293\n"
     ]
    }
   ],
   "source": [
    "for k, v in predict_dict.items():\n",
    "    predict_dict[k] = [0 if i < 0 else i for i in v]\n",
    "    for _v in v:\n",
    "        if _v < 0:\n",
    "            print(_v)\n",
    "# predict_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_frame = pd.DataFrame(predict_dict)\n",
    "pred_frame.index = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_array = pd.concat([X_test, y_test, pred_frame], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_array = pd.merge(full_test_array, test_id_array, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file = SubmissionFile(\n",
    "    validation_data=full_test_array,\n",
    "    type_of_data='validation'\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75624, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file['SquaredError'] = np.square(sub_file['Target'] - sub_file['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final RMSE -->  874.6747856223318\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(np.sum(sub_file['SquaredError'])/sub_file.shape[0])\n",
    "print('Final RMSE --> ', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'Deposit', 'AccessoryRate', 'rateTypeEntity', 'RatePerUnit',\n",
      "       'DaysOnDeposit', 'MainApplicantGender', 'Age', 'Region', 'Occupation',\n",
      "       'Term', 'TotalContractValue', 'nb_payments', 'amount_paid',\n",
      "       'percent_amt_paid', 'mean_amt_paid', 'median_amt_paid', 'max_amt_paid',\n",
      "       'min_amt_paid', 'stddev_amt_paid', 'b1', 'b2', 'b3', 'b4', 'b5'],\n",
      "      dtype='object')\n",
      "Index(['MONTHLY', 'WEEKLY', 'Male', 'Mount Kenya Region', 'Nairobi Region',\n",
      "       'North Rift', 'Null', 'Nyanza', 'South Rift', 'Western',\n",
      "       'Driver/Motorbike Rider', 'Farmer', 'Government Employee', 'Labourer',\n",
      "       'Other', 'Teacher'],\n",
      "      dtype='object')\n",
      "Index(['MONTHLY', 'WEEKLY', 'Male', 'Mount Kenya Region', 'Nairobi Region',\n",
      "       'North Rift', 'Null', 'Nyanza', 'South Rift', 'Western',\n",
      "       'Driver/Motorbike Rider', 'Farmer', 'Government Employee', 'Labourer',\n",
      "       'Other', 'Teacher'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Preparing the Entire TRAIN data for approach two and fitting the model\n",
    "train_data, target_frame = create_data_with_sliding_approach(data_without_target=train_arr, \n",
    "                                                             target_data=target)\n",
    "print(train_data.columns)\n",
    "id_array = train_data[[\"ID\"]]\n",
    "train_data.drop(columns=[\"ID\"], inplace=True)\n",
    "og_frame, encoded_train, encoder_model = encode_and_drop(train_data, \"train\", None)\n",
    "print(encoded_train.columns)\n",
    "# encoded_train = encoded_train[feature_selection]\n",
    "print(encoded_train.columns)\n",
    "encoded_train = pd.concat([encoded_train, train_data[['b1', 'b2', 'b3', 'b4', 'b5']]], axis=1)\n",
    "\n",
    "model_two_obj = ModelXgBoost(train_array=encoded_train, \n",
    "                             train_target=target_frame)\n",
    "model_two_obj.train_model()  # Default h.params (Checkout the code)\n",
    "model_two = model_two_obj.trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy to impute missing regions | did not work\n",
    "most_common_region = test_set['Region'].value_counts().index[0]\n",
    "test_set.loc[test_set['Region'].isna(), 'Region'] = most_common_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aumaron/.local/share/virtualenvs/zindi_payg-FXkRANRI/lib/python3.9/site-packages/pandas/core/frame.py:4308: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n",
      "/home/aumaron/.local/share/virtualenvs/zindi_payg-FXkRANRI/lib/python3.9/site-packages/pandas/core/frame.py:4441: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n"
     ]
    }
   ],
   "source": [
    "test_id = test_set[[\"ID\"]]\n",
    "test_set.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "# Encoding and re-attaching using train encoding model\n",
    "og_frame_test, encoded_test, encoder_model = encode_and_drop(test_set, \"test\", encoder_model)\n",
    "# encoded_test = encoded_test[feature_selection]\n",
    "encoded_test = pd.concat([encoded_test, test_set[['b1', 'b2', 'b3', 'b4', 'b5']].reset_index(drop=True)], axis=1)\n",
    "\n",
    "predict_dict = dict()\n",
    "\n",
    "for col_no, predict_col in enumerate(['m1', 'm2', 'm3', 'm4', 'm5', 'm6']):\n",
    "    predict_dict[f\"m{col_no+1}_pred\"] = model_two.predict(encoded_test)\n",
    "    int_df = encoded_test[['b1', 'b2', 'b3', 'b4', 'b5']]\n",
    "    encoded_test.drop(columns=['b1', 'b2', 'b3', 'b4', 'b5'], inplace=True)\n",
    "    concatinating_df = slide_variable_window(predictor_array=int_df, \n",
    "                                             var_to_add=pd.DataFrame(predict_dict[f\"m{col_no+1}_pred\"]))\n",
    "    encoded_test = pd.concat([encoded_test, concatinating_df], axis=1)  # We add the newly created columns\n",
    "    \n",
    "# predict_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-27.82334\n",
      "-155.75896\n",
      "-93.281136\n",
      "-135.12369\n",
      "-86.5752\n",
      "-3.7002027\n",
      "-87.47784\n",
      "-94.077965\n",
      "-74.16988\n",
      "-590.0807\n",
      "-219.53244\n",
      "-151.39647\n",
      "-168.4362\n",
      "-18.941334\n",
      "-414.90723\n",
      "-63.450375\n",
      "-115.86874\n",
      "-290.13742\n",
      "-51.83174\n"
     ]
    }
   ],
   "source": [
    "for ke, va in predict_dict.items():\n",
    "    # predict_dict[ke] = [0 if i < 0 else i for i in va]\n",
    "    for _va in va:\n",
    "        if _va < 0:\n",
    "            print(_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_frame_test = pd.DataFrame(predict_dict)\n",
    "pred_frame_test.index = test_set.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_array_test = pd.concat([test_set, pred_frame_test], axis=1)\n",
    "full_test_array_test = pd.merge(full_test_array_test, test_id, how='left', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file = SubmissionFile(\n",
    "    validation_data=full_test_array_test,\n",
    "    type_of_data='test'\n",
    ").execute()\n",
    "sub_file.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file.to_csv('../../submissions/submission_approach_2_null_region_category.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_000RHRU x m1</td>\n",
       "      <td>201.791641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_000RHRU x m2</td>\n",
       "      <td>224.120941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_000RHRU x m3</td>\n",
       "      <td>225.497955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_000RHRU x m4</td>\n",
       "      <td>261.980835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_000RHRU x m5</td>\n",
       "      <td>276.416199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56011</th>\n",
       "      <td>ID_ZZOKWZJ x m2</td>\n",
       "      <td>913.030701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56012</th>\n",
       "      <td>ID_ZZOKWZJ x m3</td>\n",
       "      <td>874.852234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56013</th>\n",
       "      <td>ID_ZZOKWZJ x m4</td>\n",
       "      <td>816.791687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56014</th>\n",
       "      <td>ID_ZZOKWZJ x m5</td>\n",
       "      <td>823.228455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56015</th>\n",
       "      <td>ID_ZZOKWZJ x m6</td>\n",
       "      <td>801.894104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56016 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID      Target\n",
       "0      ID_000RHRU x m1  201.791641\n",
       "1      ID_000RHRU x m2  224.120941\n",
       "2      ID_000RHRU x m3  225.497955\n",
       "3      ID_000RHRU x m4  261.980835\n",
       "4      ID_000RHRU x m5  276.416199\n",
       "...                ...         ...\n",
       "56011  ID_ZZOKWZJ x m2  913.030701\n",
       "56012  ID_ZZOKWZJ x m3  874.852234\n",
       "56013  ID_ZZOKWZJ x m4  816.791687\n",
       "56014  ID_ZZOKWZJ x m5  823.228455\n",
       "56015  ID_ZZOKWZJ x m6  801.894104\n",
       "\n",
       "[56016 rows x 2 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zindi_payg",
   "language": "python",
   "name": "zindi_payg"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}